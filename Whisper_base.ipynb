{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import whisper\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_whisper_model(model_size=\"base\"):\n",
    "    return whisper.load_model(model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of files in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_in_directory(folder_path, file_extension):\n",
    "    return sorted([f for f in os.listdir(folder_path) if f.endswith(file_extension)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load transcription data from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcription_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get matching transcription from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_transcription(transcription_data, sample_name):\n",
    "    return next((item['transcription'] for item in transcription_data if item['sample_name'] == sample_name), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcribe audio and measure time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio_with_time(model, wav_file_path):\n",
    "    start_time = time.time()\n",
    "    result = model.transcribe(wav_file_path)\n",
    "    end_time = time.time()\n",
    "    transcription_time = end_time - start_time\n",
    "    return result['text'], transcription_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Word Error Rate (WER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer(reference, hypothesis):\n",
    "    return jiwer.wer(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Character Error Rate (CER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cer(reference, hypothesis):\n",
    "    return jiwer.cer(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Sentence Error Rate (SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ser(reference_sentences, hypothesis_sentences):\n",
    "    incorrect_sentences = sum([ref != hyp for ref, hyp in zip(reference_sentences, hypothesis_sentences)])\n",
    "    return incorrect_sentences / len(reference_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(reference, hypothesis):\n",
    "    reference = [reference.split()]\n",
    "    hypothesis = hypothesis.split()\n",
    "    return sentence_bleu(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_levenshtein(reference, hypothesis):\n",
    "    return Levenshtein.distance(reference, hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(reference, hypothesis):\n",
    "    vectorizer = TfidfVectorizer().fit_transform([reference, hypothesis])\n",
    "    vectors = vectorizer.toarray()\n",
    "    cos_sim = cosine_similarity(vectors)[0, 1]\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to write results to a CSV file using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_csv_pandas(csv_file_path, results):\n",
    "    df = pd.DataFrame(results, columns=[\"File\", \"Whisper Output\", \"Correct Transcription\", \"WER\", \"CER\", \"SER\", \"BLEU\", \"Levenshtein Distance\", \"Cosine Similarity\", \"Transcription Time\"])\n",
    "    df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process files and save the result to CSV with tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_and_save_to_csv_pandas(folder_path, model, csv_file_path):\n",
    "    # Get list of files\n",
    "    wav_files = get_files_in_directory(folder_path, '.wav')\n",
    "    json_files = get_files_in_directory(folder_path, '.json')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=\"Processing JSON files\", unit=\"file\"):  # Add progress bar here\n",
    "        # Load the transcription data from JSON\n",
    "        json_path = os.path.join(folder_path, json_file)\n",
    "        transcription_data = load_transcription_data(json_path)\n",
    "        \n",
    "        base_name = json_file.replace(\"_transcription.json\", \"\")\n",
    "        \n",
    "        for i in tqdm(range(1, 6), desc=f\"Processing {json_file}\", leave=False, unit=\"sample\"):  # Inner loop progress bar\n",
    "            wav_file = f\"{base_name}_{i}.wav\"\n",
    "            wav_path = os.path.join(folder_path, wav_file)\n",
    "            \n",
    "            if not os.path.exists(wav_path):\n",
    "                print(f\"Audio file {wav_file} not found.\")\n",
    "                continue\n",
    "            \n",
    "            sample_name = f\"sample_{i}\"\n",
    "            correct_transcription = get_matching_transcription(transcription_data, sample_name)\n",
    "            \n",
    "            if not correct_transcription:\n",
    "                print(f\"No matching transcription found for {wav_file} in {json_file}.\")\n",
    "                continue\n",
    "            \n",
    "            # Transcribe the audio and measure time\n",
    "            whisper_output, transcription_time = transcribe_audio_with_time(model, wav_path)\n",
    "            \n",
    "            # Calculate WER, CER, SER, BLEU, Levenshtein Distance, and Cosine Similarity\n",
    "            wer = calculate_wer(correct_transcription, whisper_output)\n",
    "            cer = calculate_cer(correct_transcription, whisper_output)\n",
    "            ser = calculate_ser([correct_transcription], [whisper_output])\n",
    "            bleu = calculate_bleu(correct_transcription, whisper_output)\n",
    "            levenshtein_distance = calculate_levenshtein(correct_transcription, whisper_output)\n",
    "            cosine_sim = calculate_cosine_similarity(correct_transcription, whisper_output)\n",
    "            \n",
    "            # Store result in the list\n",
    "            results.append([wav_file, whisper_output, correct_transcription, wer, cer, ser, bleu, levenshtein_distance, cosine_sim, transcription_time])\n",
    "    \n",
    "    # Write results to CSV using pandas\n",
    "    write_results_to_csv_pandas(csv_file_path, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = \"Male British (Oliver)\"\n",
    "#csv_file_path = \"Oliver_base.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = \"Female British (Madelyn)\"\n",
    "#csv_file_path = \"Madelyn_base.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"Data/voices/Female American (Nava)\"\n",
    "csv_file_path = \"Nava_base.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = \"Male American (Michael)\"\n",
    "#csv_file_path = \"Michael_base.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Conda\\envs\\whisper\\lib\\site-packages\\whisper\\__init__.py:146: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = load_whisper_model(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress the FP16 warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"FP16 is not supported on CPU; using FP32 instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON files: 100%|██████████| 5/5 [00:46<00:00,  9.21s/file]\n"
     ]
    }
   ],
   "source": [
    "process_files_and_save_to_csv_pandas(folder_path, model, csv_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
